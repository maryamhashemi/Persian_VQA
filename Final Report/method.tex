\section{مدل پیشنهاد شده}
{
	در این بخش ابتدا نحوه‌ی آماده‌سازی مجموعه‌داده را توضیح می‌دهیم و سپس به شرح روش‌های پیاده‌سازی شده در این پروژه می‌پردازیم.
	
	\subsection{تهیه‌ی مجموعه‌داده}
	{
		مجموعه‌داده‌ای که برای حل این مسئله انتخاب کردیم؛ مجموعه داده
		 \href{https://visualqa.org/vqa_v1_download.html}{\lr{VQA v1}} 
		 است. مشخصات کامل مجموعه داده را می‌توانید در جدول 1 مشاهده کنید.
		 \begin{table}
			 \begin{center}
			 	\begin{tabular}{ c c c c } 
			 		\hline
			 		& \textbf{تعداد تصاویر} & \textbf{تعدادسوالات} & \textbf{تعداد پاسخ‌ها} \\
			 		\hline \hline
			 		\textbf{داده‌های آموزشی} & 82,783 & 248,349 &  2,483,490 \\
					
					\textbf{داده‌های ارزیابی} & 40,504 & 121,512 & 1,215,120 \\
					
					\textbf{داده‌های تست} & 81,434 & 244,302 & \\
					\hline
			 	\end{tabular}
			 \end{center}
		 \caption{مشخصات مجموعه‌داده VQA}
	 	\end{table}
		برای ترجمه مجموعه‌داده از دو ابزار Google‌ و ترگمان استفاده کردیم. در این مجموعه‌داده برای هر تصویر سه سوال وچود دارد و برای هر سوال 10 پاسخ موجود می‌باشد. در این مجموعه‌داده سه نوع سوال وجود دارد. نوع اول بله و خیر است. نوع دوم تعداد یک شی در تصویر است و نوع سوم مربوط به سوالات دیگر است. توزیع طول سوالات و پاسخ‌ها و 30 پاسخ پرتکرار در این مجموعه‌داده را برای دو حالت ترجمه Google‌‌ و ترگمان را در شکل های 
		\ref{fig:9}
		،
		\ref{fig:10}
		و 
		\ref{fig:11}
		می‌توانید مشاهده کنید. 
	}
	\subsection{\lr{LSTM Q + norm I} \cite{antol2015vqa}}
	{
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.4]{images/Vanilla_Network.jpg}
			\caption{ساختار کلی روش \lr{LSTM Q + norm I}  .}
			\label{fig:3}
		\end{figure}
		این روش ساده‌ترین روش یادگیری عمیق برای حل مسئله پرسش و پاسخ تصویری است. در اینجا مسئله VQA به عنوان یک مسئله طبقه‌بندی در نظر گرفته می‌شود که در آن 1000 پاسخ پرتکرار به عنوان کلاس‌ها انتخاب می‌شوند.  ساختار کلی این شبکه در شکل 
		\ref{fig:3}
		نشان داده شده است. ابتدا با عبور دادن تصاویر از شبکه 
		\lr{VGG19}
		 برای هر تصویر یک بردار ویژگی 4096تایی در لایه‌ی ماقبل آخر در شبکه‌ی
		  \lr{VGG19}
		   تولید می‌شود. از طرفی دیگر با عبور سوال‌ها از لایه‌یEembedding برای هر کلمه موجود در سوال یک بردار 300تایی تولید می‌شود. سپس از طریق 2 لایه LSTM بردار ویژگی معنایی سوال استخراج  می‌شود. هر یک از بردارهای ویژگی تصویر و سوال را به یک لایه‌ی Dense  1024 واحدی می‌دهیم تا ابعاد بردار‌ها مشابه هم شوند. برای ترکیب بردار ویژگی سوال و تصویر از ضرب نقطه‌ای استفاده می‌کنیم. از این بردار ترکیب شده به عنوان ورودی برای لایه‌ی کاملا متصل استفاده می‌کنیم و در نهایت با عبور از یک لایه softmax کلاس(پاسخ) پیش‌بینی شده بدست می‌آید.
	}

	\subsection{\lr{Stacked Attention Network} \cite{yang2016stacked} }
	{
		ایده‌ی اصلی روش SAN‌ این است که ابتدا از سوال، یک  بازنمایی معنایی و مفهومی استخراج می‌کند. سپس از آن به عنوان یک کوئری برای پیدا کردن مناطقی از تصویر که مرتبط با سوال است؛ استفاده می‌کند. غالباً در مسئله VQA نیاز است تا چندین مرحله استدلال صورت بگیرد. بنابراین در این شبکه از چندین لایه برای جستجو در تصویر استفاده می‌کنیم تا به تدریج به جواب مورد نظر برسیم. ساختار کلی شبکه SAN را در شکل 
		\ref{fig:1}
		می‌توانید مشاهده کنید. شبکه SAN از سه جز اصلی تشکیل شده است: 1) مدل تصویر که با استفاده از CNN ویژگی‌های سطح بالایی را از تصویر استخراج می‌کند. 2) مدل سوال که با استفاده از CNN یا LSTM ویژگی‌های معنایی سوال را استخراج می‌کند. 3) مدل
		 \lr{stacked attention}
		 ‌ که از طریق استدلال چند مرحله‌ای مناطقی از تصویر که مرتبط به سوال است را پیدا می‌کند تا پاسخ را پیش‌بینی کند.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.5]{images/san.jpg}
			\caption{ساختار کلی روش SAN .}
			\label{fig:1}
		\end{figure}
	
		\subsubsection{مدل تصویر}
		{
			در این بخش برای استخراج ویژگی از شبکه‌ی
			 \lr{VGG16}
			 ‌ استفاده می‌کنیم و ویژگی‌ها را از آخرین لایه‌ی
			 \lr{pooling}
			  ‌شبکه بدست می‌آوریم. ابتدا تمام تصاویر را به
			 \lr{448×448}
			  تغییر سایز می‌دهیم و بعد از این که تابع پیش‌پردازش موجود برای شبکه‌ی 
			 \lr{VGG16}
			   را بر روی تصاویر اعمال کردیم، تصاویر را برای استخراج ویژگی به شبکه می‌دهیم. بنابراین برای هر تصویر یک ویژگی با ابعاد
			 \lr{512×14×14}
			 حاصل می‌شود. در حقیقت، برای هر تصویر به تعداد
			 \lr{14×14}
			 منطقه استخراج می‌شود که هر منطقه به وسیله‌ی یک بردار ویژگی 512تایی بازنمایی می‌شود. برای راحتی، از یک لایه‌ی
			 \lr{Dense}
			 بعد از شبکه‌ی 
			 \lr{VGG16}
			استفاده می‌کنیم تا ابعاد بردار ویژگی مناطق مشابه با ابعاد بردار ویژگی سوال شود.
		}
	
		\subsubsection{مدل سوال}
		{
			\begin{figure}
				\centering
				\includegraphics[scale=0.5]{images/san_cnn.jpg}
				\caption{مدل سوال براساس CNN .}
				\label{fig:2}
			\end{figure}
			برای استخراج ویژگی‌های معنایی از سوال، از هر دو روش  LSTM‌ و CNN یک بعدی استفاده می‌کنیم. در هر دو روش ابتدا سوال را به یک دنباله‌ی عددی تبدیل می‌کنیم و سپس این دنباله‌ها را به یک لایه‌ی Embedding‌ می‌دهیم. در روش
			 LSTM
			خروجی لایه Embedding را به دو لایه‌ی LSTM می‌دهیم و خروجی آخرین لایه‌ی مخفی LSTM را به عنوان بردار ویژگی سوال در نظر می‌گیریم. در روش CNN خروجی Embedding را به سه لایه‌ی کانولوشنی یک بعدی با فیلترهایی با سایز 1، 2، 3 می‌دهیم که به ترتیب ترکیب‌های یک کلمه‌ای ، دو کلمه‌ای و سه کلمه‌ای را برای ما استخراج می‌کند. در نهایت بر روی خروجی هر سه لایه تابع maxpooling  را اعمال می‌کنیم و با قرار دادن این سه خروجی در کنار هم بردار ویژگی سوال بدست می‌آید. شکل 
			\ref{fig:2}
			مدل سوال بر اساس CNN را نشان می‌دهد.
		}
	
		\subsubsection{مدل \lr{stacked attention}}
		{
			در این بخش، مدل
			\lr{stacked attention}
			 با توجه به ماتریس ویژگی تصویر و بردار ویژگی سوال پاسخ را از طریق استدلال چند مرحله‌ای پیش‌بینی می‌کند. در بسیاری  از موارد، یک پاسخ فقط مربوط به یک ناحیه کوچک از تصویر است. بنابراین، استفاده از یک ماتریس ویژگی کلی  برای تصویر می‌تواند به دلیل وجود نویزهای مناطق بی‌ربط  به پاسخ، منجر به نتایج نامطلوبی شود. در عوض، استدلال از طریق چندین لایه توجه، قادر است به تدریج مناطق غیرمرتبط با جواب را فیلتر کند و از ماتریس ویژگی تصویر حذف کند. بدین منظور ماتریس ویژگی تصویر
		 $v_I$
	    و بردار ویژگی سوال
	    $v_Q$
	    ، را به یک لایه Dense می‌دهیم و خروجی این لایه را به یک تابع softmax می‌دهیم تا توزیع توجه را بر روی نواحی تصویر بدست آوریم. بنابراین داریم:
	    \begin{equation}
	    	h_A = tanh (W_{I,A} v_I \oplus (W_{Q,A}v_Q + b_A))
	    \end{equation}
	    \begin{equation}
	    	p_I = softmax(W_Ph_A + b_P)
	    \end{equation}
	    
	    بر اساس توزیع توجه
	     $p_i$
	     ، جمع وزن‌دار بردارهای تصویر را که هر کدام متناظر به یک منطقه هست را محاسبه می‌کنیم. سپس
	     $\tilde{v}_I$
	      را با بردار ویژگی سوال ترکیب می‌کنیم و یک کوئری برای لایه‌ی بعدی توجه ایجاد می‌کنیم.	    
	     \begin{equation}
	      	\tilde{v}_I =\sum_i p_iv_i,
	     \end{equation}
	     \begin{equation}
	        u =\tilde{v}_I + v_Q.
	     \end{equation}
	      این روش را به تعداد k بار تکرار می‌کنیم. در نهایت از u در لایه‌ی k برای پیش‌بینی پاسخ استفاده می‌کنیم:
	     \begin{equation}
	     	p_{ans} =softmax(W_uu^K + b_u)
	     \end{equation}
	     
		}
	}

	\subsection{\lr{HieCoAttention} \cite{lu2016hierarchical}}
	{
		\begin{figure}
			\centering
			\includegraphics[scale=0.6]{images/hiecoattention.JPG}
			\caption{ساختار کلی روش HieCoAttention}
			\label{fig:7}
		\end{figure}
		روش پیشنهاد شده در 
		\cite{lu2016hierarchical}
		دارای دو ویژگی مهم است. ویژگی اول بازنمایی سلسله‌مراتبی سوال و ویژگی دوم مکانیزم coattention می‌باشد. در ادامه این دو خصوصیات را شرح می‌دهیم.
		
		\subsubsection{بازنمایی سلسله‌مراتبی سوال}
		{
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.5]{images/hierarchical.jpg}
				\caption{بازنمایی سلسله‌مراتبی سوال.}
				\label{fig:4}
			\end{figure}
		
			در این بخش برای هر سوال سه سطح Embedding را محاسبه می‌کنیم. اولین Embedding مربوط به کلمات است که بعد از این‌که سوال را به دنباله‌های عددی تبدیل کردیم؛ با عبور دادن این دنباله‌ها از لایه‌ی Embedding ، بردارهای Embedding‌ کلمات بدست می‌آید. برای محاسبه سطح بعدی Embedding که مربوط به عبارات است از کانولوشن‌های یک بعدی با فیلترهایی با سایز 1، 2 و 3 استفاده می‌کنیم و سپس با اعمال تابع Maxpooling بردار Embedding هر عبارت بوجود می‌آید. در نهایت از Embedding عبارات  برای محاسبه‌ی Embedding  کل سوال استفاده می‌کنیم. این کار توسط یک لایه LSTM انجام می‌شود. بنابراین  برای هر سوال به صورت سلسله‌مراتبی سه سطح Embedding کلمه، عبارت و سوال تولید می‌شود. بازنمایی سلسله‌مراتبی سوال در شکل
			\ref{fig:4}
			 به تصویر کشیده شده است.
		}
		\subsubsection{مکانیزم coattention}
		{
			\begin{figure}
				\centering
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=0.85\linewidth]{images/parallel.JPG}
					\caption{}
				\end{subfigure}%
				\begin{subfigure}[b]{0.42\textwidth}
					\centering
					\includegraphics[width=0.85\linewidth]{images/alternating.JPG}
					\caption{}
				\end{subfigure}%
				\caption
				{
					(آ) 
					\lr{parallel coattention}
					(ب)
					\lr{alternating coattention}
				}
				\label{fig:5}
			\end{figure}
			در 
			\cite{}
			 دو مکانیزم برای coattention پیشنهاد شده است که از نظر ترتیب تولید 
			\lr{attention map}
			 برای سوال و تصویر با هم تفاوت دارند. اولین مکانیزم که
			\lr{parallel coattention}
			 نامیده می‌شود، باعث تولید attention به طور همزمان برای سوال و تصویر می‌شود. به مکانیزم دوم
			\lr{alternating coattention}
			 می‌گویند که برای تولید attention‌ برای سوال و تصویر به صورت تناوبی عمل می‌کند (شکل 
			\ref{fig:5}
			 ). این مکانیزم coattention در هر سه سطح سلسله‌مراتبی سؤال اجرا می‌شوند.  در این پروژه ما از مکانیزم 
			 \lr{parallel coattention}
			 استفاده می‌کنیم. در این مکانیزم با محاسبه شباهت بین ویژگی‌های تصویر و سوال، تصویر و سؤال را به هم متصل می‌کنیم. اگر بردار ویژگی تصویر را با V و بازنمایی سوال را با Q نشان دهیم؛ ماتریس شباهت C 
			 به صورت زیر محاسبه می‌شود:
			 \begin{equation}
			 	C = tanh(Q^TW_bV)
			 \end{equation}
			 
			 پس از محاسبه ماتریس شباهت، برای محاسبه بردار وزن‌های attention برای تصویر و سوال از روابط زیر استفاده می‌کنیم:
			 \begin{equation}
			 \begin{aligned}
			 	H^v = tanh(W_vV + (W_qQ)C), \ \ \ \ \  H^q = tanh(W_qQ+ (W_vV)CT)\\
			 	a^v = softmax(w^T_{hv}H^v), \ \ \ \ \  a^q = softmax(w^T_{hq}H^q) \ \ \ \ \ \ \ \ \ \ \ \ \ 
			 \end{aligned}
			 \label{eq:1}
			 \end{equation}
			 که در عبارت 
			 \ref{eq:1}
			 $W_v$
			 ، 
			 $W_q$
			 ،
			 $w_{hv}$
			 و
			 $w_{hq}$
			 پارامترهای وزن هستند. 
			 $a_v$
			 و 
			 $a_q$
			 نیز به ترتیب وزن‌های attention برای تصویر و سوال هستند. با توجه به وزن‌های attention ، بردارهای توجه تصویر و سوال به وسیله‌ی جمع وزن‌دار ویژگی‌های تصویر و ویژگی‌های سوال با وزن‌های attention محاسبه می‌شوند:
			 \begin{equation}
			 \hat{v} =\sum_{n+1}^{N} a^v_nv_n,\ \ \ \ \ \hat{q} = \sum_{t=1}^{T}a^q_t q_t
			 \end{equation}
			 
		}
	
		\subsubsection{پیش‌بینی پاسخ}
		{
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.6]{images/answer.JPG}
				\caption{پیش‌بینی پاسخ}
				\label{fig:6}
			\end{figure}
		
			ما پاسخ را بر اساس coattention تصویر و سوال بدست آمده در هر سه سطح Embedding پیش‌بینی می‌کنیم. از یک پرسپترون چندلایه (MLP) استفاده می‌کنیم تا ویژگی‌های attention را همان طور که در شکل 
		\ref{fig:6}	
			نشان داده شده است؛ ترکیب کنیم.
			\begin{equation}
			\begin{aligned}
			h^w = tanh(W_w(\hat{q}^w + \hat{v}^w))\ \ \ \  \\
			h^p = tanh(W_p[\hat{q}^p + \hat{v}^p), h^w]) \\
			h^s = tanh(W_s[(\hat{q}^s + \hat{v}^s), h^p]) \\
			p = softmax(W_hh^s)  \ \ \ \ \ \ \ \ \ \ \ \ \ 
			\end{aligned}
			\end{equation}
		
			 $W_W$
			 ، 
			 $W_p$
			 ،
			 $W_s$
			  و
			 $W_h$
			  	پارامترهای وزن هستند. p احتمال پاسخ نهایی است.
		}
	}

}

