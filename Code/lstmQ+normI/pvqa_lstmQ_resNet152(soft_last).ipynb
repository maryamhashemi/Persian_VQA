{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pvqa_lstmQ_resNet152(soft-last).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HWQ4jKhkzimO",
        "1Sy-2u_i0gAh",
        "DrMIOnHy7bdA",
        "m4OteN2u22vj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamhashemi/Persian_VQA/blob/master/pvqa_lstmQ_resNet152(soft_last).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBWMpHGoxrnz",
        "colab_type": "text"
      },
      "source": [
        "### Import Prerequesties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK-PVH7YP_L4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "f9e796b9-633a-4dec-c293-30198db5d0ab"
      },
      "source": [
        "# !pip install arabic_reshaper \n",
        "# !pip install python-bidi\n",
        "# !pip install deepdish\n",
        "# !pip install h5py\n",
        "# !pip install --upgrade numpy\n",
        "# !pip install --upgrade tables\n",
        "!pip install hazm\n",
        "!pip install h5py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.6MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 14.6MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394473 sha256=54b54f62deffa28e09d52adc613fc7d40688954b18b5882b7b3670699e7c2c5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=155248 sha256=05162a87ba1005b24b9848bdc062cfc777f96f2bb70444c13a50e0fbbcc8a48c\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6bRyFIkFhGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "# import deepdish as dd\n",
        "from PIL import Image\n",
        "# import arabic_reshaper\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "# from bidi.algorithm import get_display\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Multiply, Input,BatchNormalization\n",
        "\n",
        "import random as python_random\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "import tensorflow as tf\n",
        "import hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOGILD3IxLAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "37bec43c-8058-4614-9e6b-5283aa2ad60c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5xC8j0y1xk",
        "colab_type": "text"
      },
      "source": [
        "### Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVebfTOqy64y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DROPOUT_RATE = 0.5\n",
        "EMBEDDING_DIM = 300\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "SEQ_LENGTH = 26 #100\n",
        "VOCAB_SIZE = 1000\n",
        "OOV_TOK = \"<OOV>\"\n",
        "\n",
        "BASE_PATH = '/content/drive/My Drive/Persian_VQA/dataset/'\n",
        "QUESTION_TRAIN_PATH =   os.path.join(BASE_PATH, 'google-train_questions.json')#OpenEnded_mscoco_train2014_questions.json\n",
        "ANNOTATION_TRAIN_PATH = os.path.join(BASE_PATH, 'mscoco_train2014_annotations.json')\n",
        "# IMAGE_TRAIN_PATH = os.path.join(BASE_PATH, 'train_images_1000')\n",
        "IMAGE_TRAIN_PATH = os.path.join('/content/', 'train')\n",
        "\n",
        "QUESTION_VAL_PATH =   os.path.join(BASE_PATH, 'google-val_questions.json')#'OpenEnded_mscoco_val2014_questions.json'\n",
        "ANNOTATION_VAL_PATH = os.path.join(BASE_PATH, 'mscoco_val2014_annotations.json')\n",
        "# IMAGE_VAL_PATH = os.path.join(BASE_PATH, 'val_images_500')\n",
        "IMAGE_VAL_PATH = os.path.join('/content/', 'val')\n",
        "\n",
        "QUESTION_TEST_PATH =   os.path.join(BASE_PATH, '...')\n",
        "ANNOTATION_TEST_PATH = os.path.join(BASE_PATH, '...')\n",
        "# IMAGE_TEST_PATH = os.path.join(BASE_PATH, 'val_images_500')\n",
        "IMAGE_TEST_PATH = os.path.join('/content/', 'test')\n",
        "\n",
        "NUM_OF_CLASSES=1000\n",
        "NUM_OF_MOST_COMMON_ANSWERS = 1000#999\n",
        "BASE_PATH_parssoft = '/content/drive/My Drive/parssoftco_PVQA/'\n",
        "BASE_PATH_parssoft2 = '/content/drive/My Drive/parssoftco_PVQA2/'\n",
        "BASE_PATH_parssoft3 = '/content/drive/My Drive/parssoftco_PVQA3/'\n",
        "BASE_PATH_parssoft4 = '/content/drive/My Drive/parssoftco_PVQA4/'\n",
        "BASE_PATH_parssoft5 = '/content/drive/My Drive/parssoftco_PVQA5/'\n",
        "BASE_PATH_parssoft6 = '/content/drive/My Drive/parssoftco_PVQA6/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJg7-dXsa8i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "from zipfile import ZipFile\n",
        "# #If the downloaded file is a zip file than you can use below function to unzip it.\n",
        "def unzip(dir,where):\n",
        "    with ZipFile(dir) as zipf:\n",
        "        zipf.extractall(where)\n",
        "    print(\"File Unzipped!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqyLy7lGxky7",
        "colab_type": "text"
      },
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFFjq4jWHgKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_common_answer_from_train(answers,num_of_common_answers):\n",
        "  counts = {}\n",
        "  for ans in answers:\n",
        "      counts[ans] = counts.get(ans,0) + 1\n",
        "  counter = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
        "  most_common_vocab = []\n",
        "  for i in range(num_of_common_answers):\n",
        "      most_common_vocab.append(counter[i][1])\n",
        "  return most_common_vocab#,counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFw0Yqw8NWJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_dataset(qus, ann, answertoindex):\n",
        "  qs = []\n",
        "  # raw_answers = []\n",
        "  answers = []\n",
        "  qs_id = []\n",
        "  im_id = []\n",
        "\n",
        "  # ann2=ann\n",
        "  # for ann in ann['annotations']:\n",
        "  #   raw_answers.append(ann['multiple_choice_answer'])\n",
        "  # most_common_answers = most_common_answer_from_train(raw_answers,NUM_OF_MOST_COMMON_ANSWERS)\n",
        "  # answertoindex = {w:i+1 for i,w in enumerate(most_common_answers)}\n",
        "  # indextoanswer = {i+1:w for i,w in enumerate(most_common_answers)}\n",
        "\n",
        "  filtered_train_question_ids={}\n",
        "  # ann=ann2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  i=0\n",
        "  for ann in ann['annotations']:\n",
        " \n",
        "    if ann['multiple_choice_answer'] in answertoindex.keys():\n",
        "      answers.append(ann['multiple_choice_answer'])\n",
        "      # filtered_train_question_ids.append(ann['question_id'])\n",
        "      filtered_train_question_ids[ann['question_id']]=1\n",
        "      # if(i%1000==0):print(i)\n",
        "      i+=1\n",
        "\n",
        "  for q in qus['questions']:\n",
        "    if q['question_id'] in filtered_train_question_ids.keys():\n",
        "      qs.append(q['question'])\n",
        "      qs_id.append(q['question_id'])\n",
        "      im_id.append(q['image_id'])\n",
        "\n",
        "\n",
        "  return qs, answers, qs_id, im_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJBqQMXBwtuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(qus, ann):\n",
        "  qs = []\n",
        "  raw_answers = []\n",
        "  answers = []\n",
        "  qs_id = []\n",
        "  im_id = []\n",
        " \n",
        "\n",
        "  for q in qus['questions']:\n",
        "    qs.append(q['question'])\n",
        "    qs_id.append(q['question_id'])\n",
        "    im_id.append(q['image_id'])\n",
        "\n",
        "  for ann in ann['annotations']:\n",
        "    answers.append(ann['multiple_choice_answer'])\n",
        "\n",
        "\n",
        "  return qs, answers, qs_id, im_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTqIiXJnK6Uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_dataset():\n",
        "  qs = json.load( open(QUESTION_TRAIN_PATH))\n",
        "  ann = json.load( open(ANNOTATION_TRAIN_PATH))\n",
        "\n",
        "  original_ann = ann\n",
        "  raw_answers = []\n",
        "  for ann in ann['annotations']:\n",
        "    raw_answers.append(ann['multiple_choice_answer'])\n",
        "  most_common_answers = most_common_answer_from_train(raw_answers,NUM_OF_MOST_COMMON_ANSWERS)#NUM_OF_MOST_COMMON_ANSWERS\n",
        "  answertoindex = {w:i for i,w in enumerate(most_common_answers)}#i+1\n",
        "  indextoanswer = {i:w for i,w in enumerate(most_common_answers)}#i+1\n",
        "\n",
        "  qs, answers, qs_id, im_id=create_train_dataset(qs, original_ann, answertoindex)\n",
        "\n",
        "  return qs, answers, qs_id, im_id,answertoindex,indextoanswer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jenrLGl03rdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_val_dataset():\n",
        "  qs = json.load( open(QUESTION_VAL_PATH))\n",
        "  ann = json.load( open(ANNOTATION_VAL_PATH))\n",
        "  \n",
        "  return create_dataset(qs, ann)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ6eQ8HOPI1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_dataset():\n",
        "  qs = json.load( open(QUESTION_TEST_PATH))\n",
        "  # ann = json.load( open(ANNOTATION_TRAIN_PATH))\n",
        "  \n",
        "  return create_dataset(ims, qs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2zFZ8s2Wp2p",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV5MQ5WE_E0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_visualqa(qs, answer, image):\n",
        "  im = Image.open(image)  \n",
        "  plt.figure()\n",
        "  plt.imshow(im)\n",
        "  title = arabic_reshaper.reshape(qs + \"\\n\" + answer)\n",
        "  title = get_display(title) \n",
        "  plt.title(title)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def visualize_train(num):\n",
        "  show_visualqa(train_qs[num], train_answers[num], ims.get(train_image_ids[num]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aZM3F6EH3PE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "7ac87ed1-e0ad-4bb6-beb0-50f4f661d8a4"
      },
      "source": [
        "train_qs, train_answers, train_q_ids, train_image_ids, answertoindex, indextoanswer = get_train_dataset()\n",
        "val_qs, val_answers, val_q_ids, val_image_ids = get_val_dataset()\n",
        "# test_qs, test_answers, test_q_ids, test_image_ids = get_test_dataset()\n",
        "\n",
        "print('train : ')\n",
        "print(len(train_qs))\n",
        "print(len(train_answers))\n",
        "print(len(train_q_ids))\n",
        "print(len(train_image_ids))\n",
        "\n",
        "print('val : ')\n",
        "print(len(val_qs))\n",
        "print(len(val_answers))\n",
        "print(len(val_q_ids))\n",
        "print(len(val_image_ids))\n",
        "\n",
        "# print(len(test_qs))\n",
        "# print(len(test_answers))\n",
        "# print(len(test_q_ids))\n",
        "# print(len(test_image_ids))\n",
        "\n",
        "#train : 215359   val: 121512\n",
        "#train : 215375   val: 121512 ours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : \n",
            "215375\n",
            "215375\n",
            "215375\n",
            "215375\n",
            "val : \n",
            "121512\n",
            "121512\n",
            "121512\n",
            "121512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlzDNNBr7RIh",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare questions to feed into network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQ4jKhkzimO",
        "colab_type": "text"
      },
      "source": [
        "##### Downloads and unzips"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z_h_urH1Y31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!wget https://github.com/HaniehP/PersianNER/blob/master/glove300d.txt.zip #persian glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASeCtAsBFP3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unzip(BASE_PATH_parssoft3+'glove300d.txt.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3dTQ7MhRyeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip /content/glove.6B.zip\n",
        "# unzip('/content/glove.6B.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'\n",
        "unzip(BASE_PATH_parssoft3+'glove300d.txt.zip','/content/')#'/content/drive/My Drive/parssoftco_PVQA/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGzP7nNujxhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "file = open('glove300d.txt')#BASE_PATH_parssoft+'glove.6B.300d.txt'\n",
        "for line in (file):\n",
        "    value = line.split(' ')\n",
        "    word = value[0]\n",
        "    coef = np.array(value[1:],dtype = 'float32')\n",
        "    vocab_and_vectors[word] = coef\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO6vIV6FW_kL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3f2dac57-87bc-4362-c28f-4fc32e1ec943"
      },
      "source": [
        "\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-13 03:59:33--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258183862 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fa.300.vec.gz’\n",
            "\n",
            "cc.fa.300.vec.gz    100%[===================>]   1.17G  46.4MB/s    in 29s     \n",
            "\n",
            "2020-08-13 04:00:03 (40.8 MB/s) - ‘cc.fa.300.vec.gz’ saved [1258183862/1258183862]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCA_NXVdXHXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8553c00b-e151-4936-9f02-940596aae0a4"
      },
      "source": [
        "!gunzip /content/cc.fa.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vfRTWXRIanN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ew-TPnOIhPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip /content/cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Mk45bIKCod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/cc.fa.300.vec /content/drive/My\\ Drive/parssoftco_PVQA3/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTeDwyR9Lc26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ebef2452-97e5-44b2-a65e-008c28d6392c"
      },
      "source": [
        "file = open('cc.fa.300.vec', encoding=\"utf8\")#BASE_PATH_parssoft3+'cc.en.300.vec'\n",
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values [0]#.decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2a1ce5115840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc.fa.300.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#BASE_PATH_parssoft3+'cc.en.300.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_and_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# put words as dict indexes and vectors as words values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.fa.300.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve6QIMugr6_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "# get the vectors\n",
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz'))\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "# Now let’s prepare this file for vector extraction.\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values[0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHxWq68GMvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dd.io.save(BASE_PATH_parssoft +'fasttext-en-300.h5', vocab_and_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7vaKrFB7EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_and_vectors=dd.io.load(BASE_PATH_parssoft +'fasttext-en-300.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwsFvm_QqqVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#persian fasttext\n",
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "# get the vectors\n",
        "file_fa = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz'))\n",
        "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "# Now let’s prepare this file for vector extraction.\n",
        "\n",
        "\n",
        "\n",
        "# import codecs\n",
        "# f = codecs.open('unicode.rst', encoding='utf-8')\n",
        "# for line in f:\n",
        "#     print (repr(line))\n",
        "\n",
        "vocab_and_vectors_fa = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file_fa:\n",
        "  values = line.split()\n",
        "  # print(values)\n",
        "  word = values [0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors_fa[word] = vector\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HFXksgz1AO",
        "colab_type": "text"
      },
      "source": [
        "##### making texts and embedding ready "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8teUMJy7aWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = hazm.Normalizer()\n",
        "# num_words = VOCAB_SIZE,\n",
        "\n",
        "\n",
        "train_qs = [normalizer.normalize(item) for item in train_qs]\n",
        "val_qs = [normalizer.normalize(item) for item in val_qs]\n",
        "\n",
        "train_qs = [item.replace('؟',' ؟') for item in train_qs]\n",
        "val_qs = [item.replace('؟',' ؟') for item in val_qs]\n",
        "\n",
        "\n",
        "\n",
        "# !\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n\n",
        "tokenizer = Tokenizer(oov_token=OOV_TOK)#filters=\"-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=\"\n",
        "tokenizer.fit_on_texts(train_qs)\n",
        "\n",
        "train_X_seqs = tokenizer.texts_to_sequences(train_qs)\n",
        "val_X_seqs = tokenizer.texts_to_sequences(val_qs)\n",
        "# test_X_seqs = tokenizer.texts_to_sequences(test_qs)\n",
        "\n",
        "train_X_seqs = pad_sequences(train_X_seqs, maxlen=SEQ_LENGTH, padding='pre')#post\n",
        "val_X_seqs = pad_sequences(val_X_seqs, maxlen=SEQ_LENGTH, padding='pre')#post\n",
        "# test_X_seqs = pad_sequences(test_X_seqs, maxlen=SEQ_LENGTH, padding='post')\n",
        "\n",
        "train_X_seqs = np.array(train_X_seqs)\n",
        "val_X_seqs = np.array(val_X_seqs)\n",
        "# test_X_seqs = np.array(test_X_seqs)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPNPGlbBk2-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sy-2u_i0gAh",
        "colab_type": "text"
      },
      "source": [
        "##### making embedings matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo0lkDKNlM6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for bultin keras tokenizer\n",
        "embedding_matrix = np.zeros((len(word_index)+1 , EMBEDDING_DIM))#+ 1\n",
        "for word, i in word_index.items():\n",
        "  # print(i)\n",
        "  # embedding_vector = vocab_and_vectors_fa.get(word)\n",
        "  # embedding_vector = vocab_and_vectors.get(word)\n",
        "  embedding_vector = vocab_and_vectors.get(word) #glove\n",
        "  \n",
        "  # words that cannot be found will be set to 0\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6GecXkfN1QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_and_vectors=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yxOUW-smw7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) , EMBEDDING_DIM))#+ 1\n",
        "for word, i in word_index.items():\n",
        "  # print(i)\n",
        "  # embedding_vector = vocab_and_vectors_fa.get(word)\n",
        "  # embedding_vector = vocab_and_vectors.get(word)\n",
        "  embedding_vector = vocab_and_vectors.get(word) #glove\n",
        "  \n",
        "  # words that cannot be found will be set to 0\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ZgDeyyjJ-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_old=embedding_matrix\n",
        "print(embedding_matrix_old[0]==embedding_matrix[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D9nZ3z704RJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_ft_t_keras_farsi_hazmnorm_d300_l26\n",
        "# embedding_glv_t_keras_farsi_d300_l26\n",
        "# embedding_ft_t_keras_farsi_d300_l26\n",
        "# embedding_matrix_glove_d200_l26_prepad_orig\n",
        "# embedding_matrix_glove_d300_l26\n",
        "# embedding_matrix_glove_d300_l26_prepad_originalpaper\n",
        "# embedding_matrix_glove_d300_l26_prepad_originalpaper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVI1xS3EZJqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft + 'embedding_glove_t_keras_farsi_hazmnorm_d300_l26_pre',embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1at0tCugdISS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft5 + 'embedding_ft_t_keras_farsi_hazmnorm_d300_l26_pre_1000cls',embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08kbsGgi3bs5",
        "colab_type": "text"
      },
      "source": [
        "##### load embedings matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF2x34lK0Jxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.load(BASE_PATH_parssoft + 'embedding_ft_t_keras_farsi_hazmnormok_d300_l26_pre_1000cls.npy')\n",
        "# embedding_matrix = np.load(BASE_PATH_parssoft + 'embedding_ft_t_keras_farsi_quem_d300_l26_pre_1000cls.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrMIOnHy7bdA",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare images to feed into network - (if train_X_ims and val_X_ims havent been trained yet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iST5QjfB9-iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG19 tra\n",
        "train_images_dict = dd.io.load('/content/drive/My Drive/parssoftco_PVQA3/train_full_features_VGG19_dict.h5')\n",
        "# val_images_dict = dd.io.load('/content/drive/My Drive/parssoftco_PVQA3/val_full_features_VGG19_dict.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM6YU7FwI2Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VGG19 pre\n",
        "train_images_dict = dd.io.load(BASE_PATH_parssoft+'train_imid_to_feats_dict.h5')\n",
        "val_images_dict = dd.io.load(BASE_PATH_parssoft+'val_imid_to_feats_dict.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RELjtuYPpPvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tem = np.sqrt(np.sum(np.multiply(val_X_ims, val_X_ims), axis=1))\n",
        "t=np.tile(tem,(4096,1))\n",
        "val_X_ims = np.divide(val_X_ims, np.transpose(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRoYCSV2tZNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tem = np.sqrt(np.sum(np.multiply(train_X_ims, train_X_ims), axis=1))\n",
        "t=np.tile(tem,(4096,1))\n",
        "train_X_ims = np.divide(train_X_ims, np.transpose(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOI-ueCs-yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO9LTTruMo9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims = [train_images_dict[id] for id in train_image_ids]\n",
        "train_X_ims = np.array(train_X_ims)\n",
        "\n",
        "# val_X_ims = [val_images_dict[id] for id in val_image_ids]\n",
        "# val_X_ims = np.array(val_X_ims)\n",
        "\n",
        "\n",
        "train_images_dict =0\n",
        "val_images_dict =0\n",
        "train_ims =None\n",
        "val_ims =None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm4VToqlAYMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_dict=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axsQ85wULHCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(BASE_PATH_parssoft5+ 'train_X_ims_us_l2_375.npy',train_X_ims)\n",
        "# np.save('val_X_ims_us_l2_359.npy',val_X_ims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BXnpvyoKTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/val_X_ims_us_l2_359.npy  /content/drive/My\\ Drive/parssoftco_PVQA/val_X_ims_us_l2_359.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXATIPi2Ajto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_X_ims=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BIbe04rytTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/train_X_ims_us_l2_359.npy  /content/drive/My\\ Drive/parssoftco_PVQA/train_X_ims_us_l2_359.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKGnk5df3z8x",
        "colab_type": "text"
      },
      "source": [
        "####load train_X_ims and val_X_ims"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55FZjAKdCgsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_ims = np.load(BASE_PATH_parssoft3 + 'train_X_ims_us_375_resnet152.npy')#, mmap_mode='r'  train_X_ims_pre_l2\n",
        "val_X_ims = np.load(BASE_PATH_parssoft3 + 'val_X_ims_us_375_resnet152.npy')#, mmap_mode='r' val_X_ims_pre_l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzbbhC5n8CYo",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare labels of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl1K60q8J87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac3bbbb9-43ba-465b-be69-74f7a6010e45"
      },
      "source": [
        "train_answer_indices = []\n",
        "val_answer_indices = []\n",
        "test_answer_indices = []\n",
        "\n",
        "for ans in train_answers:\n",
        "    train_answer_indices.append(answertoindex[ans])\n",
        " \n",
        "\n",
        "for ans in val_answers:\n",
        "  if ans in answertoindex.keys():\n",
        "    val_answer_indices.append(answertoindex[ans])\n",
        "  else: \n",
        "    val_answer_indices.append('1000')\n",
        "\n",
        "\n",
        "train_Y = to_categorical(np.array(train_answer_indices),num_classes=NUM_OF_CLASSES+1)#NUM_OF_CLASSES-1\n",
        "val_Y = to_categorical(np.array(val_answer_indices))\n",
        "\n",
        "\n",
        "print(train_Y.shape)\n",
        "print(val_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(215375, 1001)\n",
            "(121512, 1001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfX_W-R74nJH",
        "colab_type": "text"
      },
      "source": [
        "### Build Vanilla Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_4z0GKrA4Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initializer=tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08)\n",
        "image_input_dim = 2048 #4096\n",
        "def img_model(img_input):\n",
        "  x = Dense(1024, activation='tanh', input_dim = image_input_dim,kernel_initializer=initializer)(img_input)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM29KNZPBCMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "def qs_model(qs_input, num_words, embedding_dim, dropout_rate):\n",
        "  x = Embedding(len(word_index)+1, embedding_dim, input_length=SEQ_LENGTH, #len(word_index)+1\n",
        "   weights=[embedding_matrix],trainable = False,embeddings_initializer=initializer)(qs_input)#weights=[embedding_matrix],\n",
        "  x = LSTM(units=512, return_sequences=True, input_shape= (None,embedding_dim),kernel_initializer=initializer,recurrent_dropout=0.5)(x)#,dropout=0.1\n",
        "  x = BatchNormalization(center=False,scale=False)(x)\n",
        "  x = LSTM(units=512, return_sequences=False,kernel_initializer=initializer,recurrent_dropout=0.5)(x)#,dropout=0.2 #,recurrent_dropout=0.5\n",
        "  x = BatchNormalization(center=False,scale=False)(x)\n",
        "  x = Dense(1024, activation='tanh',kernel_initializer=initializer)(x)\n",
        "  x = BatchNormalization(center=False,scale=False)(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP_NNUBE4uoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vanilla(num_classes, dropout_rate, num_words, embedding_dim):\n",
        "  qs_input = Input(shape=( SEQ_LENGTH ,))#train_X_seqs.shape[1]\n",
        "  img_input = Input(shape=(image_input_dim,))\n",
        "\n",
        "  CNN_model = img_model(img_input)\n",
        "  LSTM_model = qs_model(qs_input, num_words, embedding_dim, dropout_rate)\n",
        "\n",
        "  x = Multiply()([CNN_model, LSTM_model])\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1000, activation='tanh',kernel_initializer=initializer)(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  output = Dense(num_classes+1, activation='softmax',kernel_initializer=initializer)(x)\n",
        "  \n",
        "  model = Model(inputs= [qs_input, img_input], outputs= output)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-EWGFhw8lo",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv5Srzj67DQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size=215375\n",
        "val_size=121512\n",
        "#215375\n",
        "#215359\n",
        "#121512\n",
        "\n",
        "#53845\n",
        "#30152\n",
        "\n",
        "train_X_seqs_final=train_X_seqs[0:train_size]\n",
        "train_X_ims_final=train_X_ims[0:train_size]\n",
        "train_Y_final=train_Y[0:train_size]\n",
        "val_X_seqs_final=val_X_seqs[0:val_size]\n",
        "val_X_ims_final=val_X_ims[0:val_size]\n",
        "val_Y_final=val_Y[0:val_size]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhcTxnt_2Du9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "n_epochs=60\n",
        "b_size=500\n",
        "opt=tf.keras.optimizers.Adadelta(lr=1)\n",
        "np.random.seed(123)\n",
        "python_random.seed(123)\n",
        "tf.random.set_seed(1234)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiVUVXK6p2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_id=23\n",
        "def_base = BASE_PATH_parssoft6\n",
        "run_counter='final_model_'+str(run_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEk4i60Z4obu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d26039db-6add-46a8-88c0-aae56f465393"
      },
      "source": [
        "model=None\n",
        "# earlystop_callback = EarlyStopping(\n",
        "# monitor='val_loss',# min_delta=0.0001,\n",
        "# patience=3)\n",
        "\n",
        "earlystop_callback = EarlyStopping(\n",
        "monitor='val_accuracy',# min_delta=0.0001,\n",
        "patience=10)\n",
        "\n",
        "save_best_callback = ModelCheckpoint(filepath=def_base+run_counter+'.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "model = vanilla(NUM_OF_CLASSES, DROPOUT_RATE, len(word_index)+1, EMBEDDING_DIM)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])#'categorical_crossentropy'\n",
        "model.summary()\n",
        "history = model.fit([train_X_seqs_final,train_X_ims_final] ,\n",
        "                    train_Y_final, \n",
        "                    epochs = n_epochs, \n",
        "                    batch_size = b_size,#,,\n",
        "                    callbacks=[earlystop_callback,save_best_callback],#earlystop_callback,\n",
        "                      validation_data=([val_X_seqs_final,val_X_ims_final], val_Y_final) \n",
        "                    , shuffle=True\n",
        "                    )#shuffle=True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 26, 300)      3635400     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 26, 512)      1665024     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 26, 512)      1024        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 512)          2099200     batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 512)          1024        lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         525312      batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         2098176     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1024)         2048        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 1024)         0           dense[0][0]                      \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1000)         1025000     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1001)         1002001     dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 12,054,209\n",
            "Trainable params: 8,414,713\n",
            "Non-trainable params: 3,639,496\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/60\n",
            "431/431 [==============================] - 104s 242ms/step - loss: 3.0319 - accuracy: 0.3412 - val_loss: 3.4403 - val_accuracy: 0.3489\n",
            "Epoch 2/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 2.3023 - accuracy: 0.4031 - val_loss: 3.0847 - val_accuracy: 0.3716\n",
            "Epoch 3/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 2.0580 - accuracy: 0.4330 - val_loss: 3.0778 - val_accuracy: 0.3950\n",
            "Epoch 4/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 1.9219 - accuracy: 0.4548 - val_loss: 2.9444 - val_accuracy: 0.4103\n",
            "Epoch 5/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 1.8171 - accuracy: 0.4715 - val_loss: 2.9229 - val_accuracy: 0.4138\n",
            "Epoch 6/60\n",
            "431/431 [==============================] - 107s 248ms/step - loss: 1.7426 - accuracy: 0.4860 - val_loss: 2.9168 - val_accuracy: 0.4100\n",
            "Epoch 7/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 1.6792 - accuracy: 0.4981 - val_loss: 2.8460 - val_accuracy: 0.4285\n",
            "Epoch 8/60\n",
            "431/431 [==============================] - 108s 251ms/step - loss: 1.6289 - accuracy: 0.5068 - val_loss: 2.8637 - val_accuracy: 0.4308\n",
            "Epoch 9/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 1.5781 - accuracy: 0.5173 - val_loss: 2.8787 - val_accuracy: 0.4385\n",
            "Epoch 10/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 1.5360 - accuracy: 0.5286 - val_loss: 2.8434 - val_accuracy: 0.4374\n",
            "Epoch 11/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 1.4951 - accuracy: 0.5371 - val_loss: 2.8320 - val_accuracy: 0.4286\n",
            "Epoch 12/60\n",
            "431/431 [==============================] - 109s 252ms/step - loss: 1.4623 - accuracy: 0.5452 - val_loss: 2.8433 - val_accuracy: 0.4430\n",
            "Epoch 13/60\n",
            "431/431 [==============================] - 109s 253ms/step - loss: 1.4316 - accuracy: 0.5522 - val_loss: 2.8185 - val_accuracy: 0.4441\n",
            "Epoch 14/60\n",
            "431/431 [==============================] - 109s 254ms/step - loss: 1.3958 - accuracy: 0.5611 - val_loss: 2.8171 - val_accuracy: 0.4454\n",
            "Epoch 15/60\n",
            "431/431 [==============================] - 110s 255ms/step - loss: 1.3678 - accuracy: 0.5682 - val_loss: 2.8211 - val_accuracy: 0.4491\n",
            "Epoch 16/60\n",
            "431/431 [==============================] - 110s 256ms/step - loss: 1.3399 - accuracy: 0.5755 - val_loss: 2.8651 - val_accuracy: 0.4545\n",
            "Epoch 17/60\n",
            "431/431 [==============================] - 109s 253ms/step - loss: 1.3125 - accuracy: 0.5833 - val_loss: 2.8514 - val_accuracy: 0.4414\n",
            "Epoch 18/60\n",
            "431/431 [==============================] - 110s 256ms/step - loss: 1.2865 - accuracy: 0.5901 - val_loss: 2.8345 - val_accuracy: 0.4552\n",
            "Epoch 19/60\n",
            "431/431 [==============================] - 111s 258ms/step - loss: 1.2601 - accuracy: 0.5972 - val_loss: 2.8758 - val_accuracy: 0.4560\n",
            "Epoch 20/60\n",
            "431/431 [==============================] - 111s 258ms/step - loss: 1.2370 - accuracy: 0.6054 - val_loss: 2.8396 - val_accuracy: 0.4576\n",
            "Epoch 21/60\n",
            "431/431 [==============================] - 110s 254ms/step - loss: 1.2082 - accuracy: 0.6120 - val_loss: 2.8570 - val_accuracy: 0.4548\n",
            "Epoch 22/60\n",
            "431/431 [==============================] - 108s 251ms/step - loss: 1.1885 - accuracy: 0.6200 - val_loss: 2.8691 - val_accuracy: 0.4483\n",
            "Epoch 23/60\n",
            "431/431 [==============================] - 107s 248ms/step - loss: 1.1672 - accuracy: 0.6263 - val_loss: 2.8818 - val_accuracy: 0.4554\n",
            "Epoch 24/60\n",
            "431/431 [==============================] - 107s 248ms/step - loss: 1.1445 - accuracy: 0.6327 - val_loss: 2.9056 - val_accuracy: 0.4505\n",
            "Epoch 25/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 1.1236 - accuracy: 0.6400 - val_loss: 2.9093 - val_accuracy: 0.4603\n",
            "Epoch 26/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 1.1003 - accuracy: 0.6467 - val_loss: 2.9099 - val_accuracy: 0.4611\n",
            "Epoch 27/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 1.0799 - accuracy: 0.6539 - val_loss: 2.9123 - val_accuracy: 0.4614\n",
            "Epoch 28/60\n",
            "431/431 [==============================] - 107s 248ms/step - loss: 1.0564 - accuracy: 0.6600 - val_loss: 2.9347 - val_accuracy: 0.4603\n",
            "Epoch 29/60\n",
            "431/431 [==============================] - 106s 247ms/step - loss: 1.0388 - accuracy: 0.6665 - val_loss: 2.9421 - val_accuracy: 0.4603\n",
            "Epoch 30/60\n",
            "431/431 [==============================] - 106s 247ms/step - loss: 1.0194 - accuracy: 0.6721 - val_loss: 2.9612 - val_accuracy: 0.4495\n",
            "Epoch 31/60\n",
            "431/431 [==============================] - 106s 247ms/step - loss: 1.0041 - accuracy: 0.6784 - val_loss: 2.9613 - val_accuracy: 0.4608\n",
            "Epoch 32/60\n",
            "431/431 [==============================] - 107s 247ms/step - loss: 0.9817 - accuracy: 0.6837 - val_loss: 2.9867 - val_accuracy: 0.4551\n",
            "Epoch 33/60\n",
            "431/431 [==============================] - 109s 253ms/step - loss: 0.9655 - accuracy: 0.6884 - val_loss: 3.0207 - val_accuracy: 0.4637\n",
            "Epoch 34/60\n",
            "431/431 [==============================] - 107s 248ms/step - loss: 0.9472 - accuracy: 0.6946 - val_loss: 3.0319 - val_accuracy: 0.4568\n",
            "Epoch 35/60\n",
            "431/431 [==============================] - 106s 245ms/step - loss: 0.9334 - accuracy: 0.6991 - val_loss: 3.0209 - val_accuracy: 0.4551\n",
            "Epoch 36/60\n",
            "431/431 [==============================] - 106s 246ms/step - loss: 0.9129 - accuracy: 0.7060 - val_loss: 3.0261 - val_accuracy: 0.4597\n",
            "Epoch 37/60\n",
            "431/431 [==============================] - 106s 246ms/step - loss: 0.8982 - accuracy: 0.7108 - val_loss: 3.0472 - val_accuracy: 0.4557\n",
            "Epoch 38/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 0.8818 - accuracy: 0.7156 - val_loss: 3.0815 - val_accuracy: 0.4629\n",
            "Epoch 39/60\n",
            "431/431 [==============================] - 107s 249ms/step - loss: 0.8668 - accuracy: 0.7204 - val_loss: 3.0793 - val_accuracy: 0.4605\n",
            "Epoch 40/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 0.8482 - accuracy: 0.7266 - val_loss: 3.0883 - val_accuracy: 0.4625\n",
            "Epoch 41/60\n",
            "431/431 [==============================] - 108s 250ms/step - loss: 0.8336 - accuracy: 0.7320 - val_loss: 3.1239 - val_accuracy: 0.4637\n",
            "Epoch 42/60\n",
            "431/431 [==============================] - 108s 251ms/step - loss: 0.8224 - accuracy: 0.7355 - val_loss: 3.1174 - val_accuracy: 0.4543\n",
            "Epoch 43/60\n",
            "431/431 [==============================] - 108s 251ms/step - loss: 0.8045 - accuracy: 0.7403 - val_loss: 3.1368 - val_accuracy: 0.4564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUQtttmPbJdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "389f8cc0-bd17-4f79-f8fd-d95ef1ce5073"
      },
      "source": [
        "!cp /content/final_model_14.h5 /content/drive/My\\ Drive/parssoftco_PVQA4/final_model_14.h5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target 'Drive/parssoftco_PVQA4/final_model_14.h5' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtaA37jxEGaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "63109b67-a910-402c-98dd-69a33db06aff"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "def plot_accuracy_loss(history, exp,def_base):\n",
        "  acc      = history.history[     'accuracy' ]\n",
        "  val_acc  = history.history[ 'val_accuracy' ]\n",
        "  loss     = history.history[    'loss' ]\n",
        "  val_loss = history.history['val_loss' ]\n",
        "\n",
        "  epochs   = range(len(acc))\n",
        "\n",
        "  plt.plot  ( epochs,     acc, label='train_acc' )\n",
        "  plt.plot  ( epochs, val_acc, label='val_acc' )\n",
        "  plt.title ('Training and validation accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.savefig(def_base + 'Training and validation accuracy' + exp + '.jpg')\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot  ( epochs,     loss, label='train_loss' )\n",
        "  plt.plot  ( epochs, val_loss, label='val_loss' )\n",
        "  plt.title ('Training and validation loss'   )\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.savefig(def_base + 'Training and validation loss' + exp + '.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l7qqvrlOKGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracy_loss(history,run_counter,def_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMo1oE_T6EJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "83a84a92-3935-4e73-8a3e-bca035660d5b"
      },
      "source": [
        "model=tf.keras.models.load_model(def_base+run_counter+'.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TpkEEXm1BYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3d808449-3ccc-45e4-ec25-31768525055b"
      },
      "source": [
        "def Evaluation(model):\n",
        "  results = model.evaluate([val_X_seqs_final,val_X_ims_final], val_Y_final, batch_size=300)\n",
        "  print('val loss, val acc:', results)\n",
        "Evaluation(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "406/406 [==============================] - 16s 40ms/step - loss: 3.0207 - accuracy: 0.4637\n",
            "val loss, val acc: [3.0206685066223145, 0.463715523481369]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou-AUdbiWHgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fa86471-30b9-4644-8c9e-c1b2fc968245"
      },
      "source": [
        "result = []\n",
        "\n",
        "p = model.predict([val_X_seqs_final,val_X_ims_final],batch_size=256)\n",
        "\n",
        "for q in range(len(val_q_ids)):\n",
        "  # if(q<val_size):\n",
        "  ans=indextoanswer[p[q].argmax(axis=-1)]\n",
        "  q_id=val_q_ids[q]\n",
        "  result.append({u'answer': ans, u'question_id': q_id})\n",
        "  # else:\n",
        "  #   ans='NAN'\n",
        "  # q_id=val_q_ids[q]\n",
        "  # result.append({u'answer': ans, u'question_id': q_id})\n",
        "\n",
        "  # if q%50000==0 :print(q)\n",
        "\n",
        "print('Saving result...')\n",
        "my_list = list(result)\n",
        "dd = json.dump(my_list,open(BASE_PATH_parssoft2+'result.json','w'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving result...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZwLZgluivjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "24d0bf36-11b6-43b4-f33d-f76d957f7b07"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "import sys\n",
        "dataDir = '/content/drive/My Drive/parssoftco_PVQA2'\n",
        "sys.path.insert(0, '/content/drive/My Drive/parssoftco_PVQA/')\n",
        "from vqaTools.vqa import VQA\n",
        "from vqaEvaluation.vqaEval import VQAEval\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# set up file names and paths\n",
        "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
        "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
        "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0. \n",
        "dataSubType ='val2014'\n",
        "annFile     ='%s/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
        "quesFile    ='%s/google-val_questions.json'%(dataDir)\n",
        "imgDir      ='%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
        "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType']\n",
        "\n",
        "# An example result json file has been provided in './Results' folder.  \n",
        "\n",
        "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] =['%s/result.json'%(dataDir) for fileType in fileTypes]\n",
        "# create vqa object and vqaRes object\n",
        "vqa = VQA(annFile, quesFile)\n",
        "vqaRes = vqa.loadRes(resFile, quesFile)\n",
        "\n",
        "# create vqaEval object by taking vqa and vqaRes\n",
        "vqaEval = VQAEval(vqa, vqaRes, n=2)   #n is precision of accuracy (number of places after decimal), default is 2\n",
        "\n",
        "# evaluate results\n",
        "\"\"\"\n",
        "If you have a list of question ids on which you would like to evaluate your results, pass it as a list to below function\n",
        "By default it uses all the question ids in annotation file\n",
        "\"\"\"\n",
        "vqaEval.evaluate() \n",
        "\n",
        "# print accuracies\n",
        "print (\"\\n\")\n",
        "print (\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
        "\n",
        "print (\"Per Answer Type Accuracy is the following:\")\n",
        "for ansType in vqaEval.accuracy['perAnswerType']:\n",
        "\tprint (\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
        "print (\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading VQA annotations and questions into memory...\n",
            "0:00:03.247617\n",
            "creating index..#.#\n",
            "index created!\n",
            "Loading and preparing results...     \n",
            "DONE (t=0.22s)\n",
            "creating index..#.#\n",
            "index created!\n",
            "computing accuracy\n",
            "Finshed Percent: [####################] 99% Done computing accuracy\n",
            "\n",
            "\n",
            "Overall Accuracy is: 53.58\n",
            "\n",
            "Per Answer Type Accuracy is the following:\n",
            "other : 40.40\n",
            "yes/no : 78.50\n",
            "number : 31.76\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}